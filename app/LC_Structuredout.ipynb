{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use of openai plus langchain for processing information in a pdf\n",
    "Generated using chatGPT for incorporating asyncio for concurrent running of prompts\n",
    "Generated by pasting my code from the analysis_v3 script with the following question:\n",
    "Can you modify the below python code to incorporate asyncio to allow concurrent running of the paper_search() function?\n",
    "\"\"\"\n",
    "from pathlib import Path  # directory setting\n",
    "import asyncio # For async asking of prompts\n",
    "from dotenv import load_dotenv, find_dotenv  # loading in API keys\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chat_models import ChatOpenAI  # LLM import\n",
    "from langchain import LLMChain  # Agent import\n",
    "from langchain.output_parsers import (  # Structuring the output format from the LLM questions\n",
    "    StructuredOutputParser,\n",
    "    ResponseSchema\n",
    ")\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.prompts.chat import ( # prompts for designing inputs\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "\n",
    "import pydantic\n",
    "from langchain.agents import AgentExecutor, initialize_agent, AgentType\n",
    "from langchain.schema import AgentFinish\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "#from ..Server.PDFDataExtractor.pdfdataextractor.demo import read_single\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/Users/desot1/Dev/automating-metadata/Server/PDFDataExtractor/pdfdataextractor\"))\n",
    "\n",
    "from demo import read_single\n",
    "from pyalex import Works, Authors, Sources, Institutions, Concepts, Publishers, Funders\n",
    "import pyalex\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# Load in API keys from .env file\n",
    "load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Make a checker that can evaluate outputs. \n",
    "- Make an agent that can take the tools - search over document, check, and return ORCHID id. \n",
    "\n",
    "STEP ONE: Agent that searches over a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_paper_search(query, docs, chain, output_parser):\n",
    "    \"\"\"\n",
    "    Async version of paper search, run question for the document concurrently with other questions\n",
    "    \"\"\"\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    out = await chain.arun(doc_text=docs, query=query, format_instructions=format_instructions)  # need to have await combined with chain.arun\n",
    "    results = output_parser.parse(out)\n",
    "    print(type(results))\n",
    "    return results\n",
    "\n",
    "\n",
    "async def langchain_paper_search(file_path):\n",
    "\n",
    "    #%% Setup, defining framework of paper info extraction\n",
    "    # Define language model to use\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "    # Structured Output Schema\n",
    "    #motivation_schema = ResponseSchema(name=\"motivation\", description=\"This is the question or challenge that the work of this paper seeks to address.\")\n",
    "    #methods_schema = ResponseSchema(name=\"methods\", description=\"This is the experimental methods and characterization techniques used by the authors in this paper.\")\n",
    "    #results_schema = ResponseSchema(name=\"results\", description=\"This is a summary of the major results and conclusions obtained in the paper.\")\n",
    "    #figure_schema = ResponseSchema(name=\"figures\", description=\"This is a comma separated list of descriptions for each figure in the paper.\")\n",
    "    #future_work_schema = ResponseSchema(name=\"future\", description=\"This is any remaining questions or future work described by the authors in the Conclusions section of the paper.\")\n",
    "    author_schema = ResponseSchema(name=\"author\", description=\"This is a list of the authors of this paper.\")\n",
    "\n",
    "\n",
    "    # Defining system and human prompts with variables\n",
    "    system_template = \"You are a world class research assistant who produces answers based on facts. \\\n",
    "                        You are tasked with reading the following publication text and answering questions based on the information: {doc_text}.\\\n",
    "                        You do not make up information that cannot be found in the text of the provided paper.\"\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)  # providing the system prompt\n",
    "\n",
    "    human_template = \"{query}. {format_instructions}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "\n",
    "    #%% Extracting info from paper\n",
    "    # Define the PDF document, load it in\n",
    "    loader = PyPDFLoader(str(file_path))  # convert path to string to work with loader\n",
    "    document = loader.load()\n",
    "\n",
    "    # Define all the queries and corresponding schemas in a list\n",
    "    queries_schemas_docs = [\n",
    "        #(\"What are the experimental methods and techniques used by the authors? This can include ways that data was collected as well as ways the samples were synthesized.\", [methods_schema], document),\n",
    "        #\"What is the scientific question, challenge, or motivation that the authors are trying to address?\", [motivation_schema], document),\n",
    "        #(\"Provide a summary of the results and discussions in the paper. What results were obtained and what conclusions were reached?\", [results_schema], document),\n",
    "        #(\"Provide a summary of each figure described in the paper. Your response should be a one sentence summary of the figure description, \\\n",
    "        # beginning with 'Fig. #  - description...', with each figure description separated by a comma. For example:'Fig. 1 - description..., Fig. 2 - description..., Fig. 3 - description...'\", [figure_schema], document),\n",
    "        #(\"What future work or unanswered questions are mentioned by the authors?\", [future_work_schema], document),\n",
    "        (\"Who is(are) the author(s) of this paper?\", [author_schema], document)\n",
    "    ]\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    # Run the queries concurrently using asyncio.gather\n",
    "    for query, schemas, docs in queries_schemas_docs:\n",
    "        output_parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "        task = async_paper_search(query, docs, chain, output_parser)\n",
    "        tasks.append(task)\n",
    "\n",
    "    summary = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Extracting individual elements from the summary\n",
    "    # title, authors, materials, methods, motive, results, figures, future, tags = summary\n",
    "    #methods, motive, results, figures, future, \n",
    "    title = summary\n",
    "\n",
    "    #llm_output = motive | methods | figures | results | future | \n",
    "    llm_output = title\n",
    "\n",
    "    return llm_output\n",
    "\n",
    "#def quality_check(input, llm): \n",
    "    \n",
    "def get_orchid(authors): \n",
    "    orchid = []\n",
    "    print(type(authors))\n",
    "    author_info = {}\n",
    "    print(authors)\n",
    "    author_list = authors[0]['author']\n",
    "    #author_list = author_string.split(', ')\n",
    "    \n",
    "    print(author_list[0])\n",
    "    for author in range(len(author_list)): \n",
    "        try: \n",
    "            url = \"https://api.openalex.org/autocomplete/authors?q=\" + author_list[author]\n",
    "        except: \n",
    "            print(\"Your author might not be registered with ORCHID\")\n",
    "        response = json.loads(requests.get(url).text)\n",
    "        \n",
    "        if response[\"meta\"][\"count\"] == 1: \n",
    "            orchid = response[\"results\"][0][\"external_id\"]\n",
    "            author_info[author_list[author]] = {orchid, response[\"results\"][0][\"hint\"]}\n",
    "        elif response[\"meta\"][\"count\"] == 0: #FAKE - Create a test so we can check if the return is valid. \n",
    "            print(\"There are no ORCHID suggestions for this author\")\n",
    "        else: \n",
    "            orchid = response[\"results\"][0][\"external_id\"]\n",
    "            author_info[author_list[author]] = {orchid, response[\"results\"][0][\"hint\"]}\n",
    "            #create an async function which ranks the authors based on the similarity to the paper. \n",
    "\n",
    "    print(author_info)\n",
    "    \n",
    "\n",
    "llm_output = get_orchid(await langchain_paper_search(\"/Users/desot1/Dev/automating-metadata/app/uploads/Navahas2018.pdf\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/embeddings/openai.py:186\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     tokens \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m docs \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39msplit_documents(pages)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m retriever \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39;49mfrom_documents(docs, embeddings)\u001b[39m.\u001b[39mas_retriever()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Wrap retrievers in a Tool\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m tools\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     Tool(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         args_schema\u001b[39m=\u001b[39mDocumentInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/vectorstores/base.py:296\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    295\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 296\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39;49mmetadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/vectorstores/faiss.py:384\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[1;32m    361\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    366\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[1;32m    367\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \n\u001b[1;32m    369\u001b[0m \u001b[39m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__from(\n\u001b[1;32m    386\u001b[0m         texts,\n\u001b[1;32m    387\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    391\u001b[0m     )\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/embeddings/openai.py:275\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/embeddings/openai.py:240\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n\u001b[1;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to for OpenAIEmbeddings. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "class DocumentInput(BaseModel):\n",
    "    question: str = Field()\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "tools = []\n",
    "files = [\n",
    "    # https://digitalassets.tesla.com/tesla-contents/image/upload/IR/TSLA-Q1-2023-Update\n",
    "    {\n",
    "        \"name\": \"navahas-research\",\n",
    "        \"path\": \"/Users/desot1/Dev/automating-metadata/app/uploads/Navahas2018.pdf\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    loader = PyPDFLoader(file[\"path\"])\n",
    "    pages = loader.load_and_split()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    retriever = FAISS.from_documents(docs, embeddings).as_retriever()\n",
    "\n",
    "    # Wrap retrievers in a Tool\n",
    "    tools.append(\n",
    "        Tool(\n",
    "            args_schema=DocumentInput,\n",
    "            name=file[\"name\"],\n",
    "            description=f\"useful when you want to answer questions about {file['name']}\",\n",
    "            func=RetrievalQA.from_chain_type(llm=llm, retriever=retriever),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "OPENAI_FUNCTIONS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-0613\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     agent\u001b[39m=\u001b[39mAgentType\u001b[39m.\u001b[39;49mOPENAI_FUNCTIONS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     tools\u001b[39m=\u001b[39mtools,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m agent({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdid alphabet or tesla have more revenue?\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/enum.py:437\u001b[0m, in \u001b[0;36mEnumMeta.__getattr__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_member_map_[name]\n\u001b[1;32m    436\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: OPENAI_FUNCTIONS"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo-0613\",\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent({\"input\": \"did alphabet or tesla have more revenue?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use of openai plus langchain for processing information in a pdf\n",
    "Generated using chatGPT for incorporating asyncio for concurrent running of prompts\n",
    "Generated by pasting my code from the analysis_v3 script with the following question:\n",
    "Can you modify the below python code to incorporate asyncio to allow concurrent running of the paper_search() function?\n",
    "\"\"\"\n",
    "from pathlib import Path  # directory setting\n",
    "import asyncio # For async asking of prompts\n",
    "from dotenv import load_dotenv, find_dotenv  # loading in API keys\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chat_models import ChatOpenAI  # LLM import\n",
    "from langchain import LLMChain  # Agent import\n",
    "from langchain.output_parsers import (  # Structuring the output format from the LLM questions\n",
    "    StructuredOutputParser,\n",
    "    ResponseSchema\n",
    ")\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.prompts.chat import ( # prompts for designing inputs\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "\n",
    "import pydantic\n",
    "from langchain.agents import AgentExecutor, initialize_agent, AgentType\n",
    "from langchain.schema import AgentFinish\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "#from ..Server.PDFDataExtractor.pdfdataextractor.demo import read_single\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/Users/desot1/Dev/automating-metadata/Server/PDFDataExtractor/pdfdataextractor\"))\n",
    "\n",
    "#from demo import read_single\n",
    "from pyalex import Works, Authors, Sources, Institutions, Concepts, Publishers, Funders\n",
    "import pyalex\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# Load in API keys from .env file\n",
    "load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Make a checker that can evaluate outputs. \n",
    "- Make an agent that can take the tools - search over document, check, and return ORCHID id. \n",
    "\n",
    "STEP ONE: Agent that searches over a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-kz8QEpvnUJuyfb0BsfrkBHU9 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-kz8QEpvnUJuyfb0BsfrkBHU9 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-kz8QEpvnUJuyfb0BsfrkBHU9 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-kz8QEpvnUJuyfb0BsfrkBHU9 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 16.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-kz8QEpvnUJuyfb0BsfrkBHU9 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for gpt-4 in organization org-kz8QEpvnUJuyfb0BsfrkBHU9 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m             \u001b[39m#create an async function which ranks the authors based on the similarity to the paper. \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39mprint\u001b[39m(author_info)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m llm_output \u001b[39m=\u001b[39m get_orchid(\u001b[39mawait\u001b[39;00m langchain_paper_search(\u001b[39m\"\u001b[39m\u001b[39m/Users/desot1/Dev/automating-metadata/report.pdf\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;32m/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     task \u001b[39m=\u001b[39m async_paper_search(query, docs, chain, output_parser)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     tasks\u001b[39m.\u001b[39mappend(task)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m summary \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Extracting individual elements from the summary\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# title, authors, materials, methods, motive, results, figures, future, tags = summary\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m#methods, motive, results, figures, future, \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m title \u001b[39m=\u001b[39m summary\n",
      "\u001b[1;32m/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mAsync version of paper search, run question for the document concurrently with other questions\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m format_instructions \u001b[39m=\u001b[39m output_parser\u001b[39m.\u001b[39mget_format_instructions()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m chain\u001b[39m.\u001b[39marun(doc_text\u001b[39m=\u001b[39mdocs, query\u001b[39m=\u001b[39mquery, format_instructions\u001b[39m=\u001b[39mformat_instructions)  \u001b[39m# need to have await combined with chain.arun\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m results \u001b[39m=\u001b[39m output_parser\u001b[39m.\u001b[39mparse(out)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(results))\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chains/base.py:266\u001b[0m, in \u001b[0;36mChain.arun\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39macall(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks))[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 266\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39macall(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks))[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    268\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but not both. Got args: \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m and kwargs: \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m )\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chains/base.py:178\u001b[0m, in \u001b[0;36mChain.acall\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    177\u001b[0m     \u001b[39mawait\u001b[39;00m run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    179\u001b[0m \u001b[39mawait\u001b[39;00m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    180\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chains/base.py:172\u001b[0m, in \u001b[0;36mChain.acall\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    166\u001b[0m run_manager \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    167\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    168\u001b[0m     inputs,\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 172\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_acall(inputs, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    173\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    174\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_acall(inputs)\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    177\u001b[0m     \u001b[39mawait\u001b[39;00m run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chains/llm.py:195\u001b[0m, in \u001b[0;36mLLMChain._acall\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_acall\u001b[39m(\n\u001b[1;32m    191\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    192\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m    193\u001b[0m     run_manager: Optional[AsyncCallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 195\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magenerate([inputs], run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chains/llm.py:90\u001b[0m, in \u001b[0;36mLLMChain.agenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maprep_prompts(input_list)\n\u001b[0;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39magenerate_prompt(\n\u001b[1;32m     91\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     92\u001b[0m )\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chat_models/base.py:151\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39magenerate_prompt\u001b[39m(\n\u001b[1;32m    145\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    146\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m    147\u001b[0m     stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    148\u001b[0m     callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    149\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    150\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 151\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magenerate(prompt_messages, stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chat_models/base.py:128\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[39mawait\u001b[39;00m run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    129\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n\u001b[1;32m    130\u001b[0m generations \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39mgenerations \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chat_models/base.py:118\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m    114\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agenerate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[1;32m    115\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\n\u001b[1;32m    119\u001b[0m         \u001b[39m*\u001b[39m[\n\u001b[1;32m    120\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agenerate(m, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    121\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    122\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agenerate(m, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    123\u001b[0m             \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages\n\u001b[1;32m    124\u001b[0m         ]\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[39mawait\u001b[39;00m run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:340\u001b[0m, in \u001b[0;36mChatOpenAI._agenerate\u001b[0;34m(self, messages, stop, run_manager)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[ChatGeneration(message\u001b[39m=\u001b[39mmessage)])\n\u001b[1;32m    339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m acompletion_with_retry(\n\u001b[1;32m    341\u001b[0m         \u001b[39mself\u001b[39m, messages\u001b[39m=\u001b[39mmessage_dicts, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:67\u001b[0m, in \u001b[0;36macompletion_with_retry\u001b[0;34m(llm, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/tenacity/_asyncio.py:88\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     87\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39masync_wrapped\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/tenacity/_asyncio.py:47\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt\u001b[39m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_attempt\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/tenacity/_asyncio.py:50\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:65\u001b[0m, in \u001b[0;36macompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:45\u001b[0m, in \u001b[0;36mChatCompletion.acreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     47\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:217\u001b[0m, in \u001b[0;36mEngineAPIResource.acreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39macreate\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    202\u001b[0m ):\n\u001b[1;32m    203\u001b[0m     (\n\u001b[1;32m    204\u001b[0m         deployment_id,\n\u001b[1;32m    205\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 217\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m requestor\u001b[39m.\u001b[39marequest(\n\u001b[1;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         url,\n\u001b[1;32m    220\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    221\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    222\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    228\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    229\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/openai/api_requestor.py:310\u001b[0m, in \u001b[0;36mAPIRequestor.arequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marequest_raw(\n\u001b[1;32m    301\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    302\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    309\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_async_response(result, stream)\n\u001b[1;32m    311\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[39mawait\u001b[39;00m ctx\u001b[39m.\u001b[39m\u001b[39m__aexit__\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/openai/api_requestor.py:645\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_async_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mexcept\u001b[39;00m aiohttp\u001b[39m.\u001b[39mClientError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    643\u001b[0m     util\u001b[39m.\u001b[39mlog_warn(e, body\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mcontent)\n\u001b[1;32m    644\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 645\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    646\u001b[0m         (\u001b[39mawait\u001b[39;49;00m result\u001b[39m.\u001b[39;49mread())\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    647\u001b[0m         result\u001b[39m.\u001b[39;49mstatus,\n\u001b[1;32m    648\u001b[0m         result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    649\u001b[0m         stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    650\u001b[0m     ),\n\u001b[1;32m    651\u001b[0m     \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    652\u001b[0m )\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/openai/api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for gpt-4 in organization org-kz8QEpvnUJuyfb0BsfrkBHU9 on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues."
     ]
    }
   ],
   "source": [
    "async def async_paper_search(query, docs, chain, output_parser):\n",
    "    \"\"\"\n",
    "    Async version of paper search, run question for the document concurrently with other questions\n",
    "    \"\"\"\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    out = await chain.arun(doc_text=docs, query=query, format_instructions=format_instructions)  # need to have await combined with chain.arun\n",
    "    results = output_parser.parse(out)\n",
    "    print(type(results))\n",
    "    return results\n",
    "\n",
    "\n",
    "async def langchain_paper_search(file_path):\n",
    "\n",
    "    #%% Setup, defining framework of paper info extraction\n",
    "    # Define language model to use\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "    # Structured Output Schema\n",
    "    #motivation_schema = ResponseSchema(name=\"motivation\", description=\"This is the question or challenge that the work of this paper seeks to address.\")\n",
    "    #methods_schema = ResponseSchema(name=\"methods\", description=\"This is the experimental methods and characterization techniques used by the authors in this paper.\")\n",
    "    #results_schema = ResponseSchema(name=\"results\", description=\"This is a summary of the major results and conclusions obtained in the paper.\")\n",
    "    #figure_schema = ResponseSchema(name=\"figures\", description=\"This is a comma separated list of descriptions for each figure in the paper.\")\n",
    "    #future_work_schema = ResponseSchema(name=\"future\", description=\"This is any remaining questions or future work described by the authors in the Conclusions section of the paper.\")\n",
    "    author_schema = ResponseSchema(name=\"author\", description=\"This is a list of the authors of this paper.\")\n",
    "\n",
    "\n",
    "    # Defining system and human prompts with variables\n",
    "    system_template = \"You are a world class research assistant who produces answers based on facts. \\\n",
    "                        You are tasked with reading the following publication text and answering questions based on the information: {doc_text}.\\\n",
    "                        You do not make up information that cannot be found in the text of the provided paper.\"\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)  # providing the system prompt\n",
    "\n",
    "    human_template = \"{query}. {format_instructions}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "\n",
    "    #%% Extracting info from paper\n",
    "    # Define the PDF document, load it in\n",
    "    loader = PyPDFLoader(str(file_path))  # convert path to string to work with loader\n",
    "    document = loader.load()\n",
    "\n",
    "    # Define all the queries and corresponding schemas in a list\n",
    "    queries_schemas_docs = [\n",
    "        #(\"What are the experimental methods and techniques used by the authors? This can include ways that data was collected as well as ways the samples were synthesized.\", [methods_schema], document),\n",
    "        #\"What is the scientific question, challenge, or motivation that the authors are trying to address?\", [motivation_schema], document),\n",
    "        #(\"Provide a summary of the results and discussions in the paper. What results were obtained and what conclusions were reached?\", [results_schema], document),\n",
    "        #(\"Provide a summary of each figure described in the paper. Your response should be a one sentence summary of the figure description, \\\n",
    "        # beginning with 'Fig. #  - description...', with each figure description separated by a comma. For example:'Fig. 1 - description..., Fig. 2 - description..., Fig. 3 - description...'\", [figure_schema], document),\n",
    "        #(\"What future work or unanswered questions are mentioned by the authors?\", [future_work_schema], document),\n",
    "        (\"Who is(are) the author(s) of this paper?\", [author_schema], document)\n",
    "    ]\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    # Run the queries concurrently using asyncio.gather\n",
    "    for query, schemas, docs in queries_schemas_docs:\n",
    "        output_parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "        task = async_paper_search(query, docs, chain, output_parser)\n",
    "        tasks.append(task)\n",
    "\n",
    "    summary = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Extracting individual elements from the summary\n",
    "    # title, authors, materials, methods, motive, results, figures, future, tags = summary\n",
    "    #methods, motive, results, figures, future, \n",
    "    title = summary\n",
    "\n",
    "    #llm_output = motive | methods | figures | results | future | \n",
    "    llm_output = title\n",
    "\n",
    "    return llm_output\n",
    "\n",
    "#def quality_check(input, llm): \n",
    "    \n",
    "def get_orchid(authors): \n",
    "    orchid = []\n",
    "    print(type(authors))\n",
    "    author_info = {}\n",
    "    print(authors)\n",
    "    author_list = authors[0]['author']\n",
    "    author_list = authors.split(', ')\n",
    "    \n",
    "    print(author_list[0])\n",
    "    for author in range(len(author_list)): \n",
    "        try: \n",
    "            url = \"https://api.openalex.org/autocomplete/authors?q=\" + author_list[author]\n",
    "        except: \n",
    "            print(\"Your author might not be registered with ORCHID\")\n",
    "        response = json.loads(requests.get(url).text)\n",
    "        \n",
    "        if response[\"meta\"][\"count\"] == 1: \n",
    "            orchid = response[\"results\"][0][\"external_id\"]\n",
    "            author_info[author_list[author]] = {orchid, response[\"results\"][0][\"hint\"]}\n",
    "        elif response[\"meta\"][\"count\"] == 0: #FAKE - Create a test so we can check if the return is valid. \n",
    "            print(\"There are no ORCHID suggestions for this author\")\n",
    "        else: \n",
    "            orchid = response[\"results\"][0][\"external_id\"]\n",
    "            author_info[author_list[author]] = {orchid, response[\"results\"][0][\"hint\"]}\n",
    "            # an async function which ranks the authors based on the similarity to the paper. \n",
    "\n",
    "    print(author_info)\n",
    "    \n",
    "\n",
    "llm_output = get_orchid(await langchain_paper_search(\"/Users/desot1/Dev/automating-metadata/report.pdf\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/embeddings/openai.py:186\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     tokens \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m docs \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39msplit_documents(pages)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m retriever \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39;49mfrom_documents(docs, embeddings)\u001b[39m.\u001b[39mas_retriever()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Wrap retrievers in a Tool\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m tools\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     Tool(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         args_schema\u001b[39m=\u001b[39mDocumentInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/vectorstores/base.py:296\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    295\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 296\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39;49mmetadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/vectorstores/faiss.py:384\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[1;32m    361\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    366\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[1;32m    367\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \n\u001b[1;32m    369\u001b[0m \u001b[39m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__from(\n\u001b[1;32m    386\u001b[0m         texts,\n\u001b[1;32m    387\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    391\u001b[0m     )\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/embeddings/openai.py:275\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[0;32m~/Dev/venv/lib/python3.10/site-packages/langchain/embeddings/openai.py:240\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n\u001b[1;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to for OpenAIEmbeddings. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "class DocumentInput(BaseModel):\n",
    "    question: str = Field()\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "tools = []\n",
    "files = [\n",
    "    # https://digitalassets.tesla.com/tesla-contents/image/upload/IR/TSLA-Q1-2023-Update\n",
    "    {\n",
    "        \"name\": \"navahas-research\",\n",
    "        \"path\": \"/Users/desot1/Dev/automating-metadata/app/uploads/Navahas2018.pdf\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    loader = PyPDFLoader(file[\"path\"])\n",
    "    pages = loader.load_and_split()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    retriever = FAISS.from_documents(docs, embeddings).as_retriever()\n",
    "\n",
    "    # Wrap retrievers in a Tool\n",
    "    tools.append(\n",
    "        Tool(\n",
    "            args_schema=DocumentInput,\n",
    "            name=file[\"name\"],\n",
    "            description=f\"useful when you want to answer questions about {file['name']}\",\n",
    "            func=RetrievalQA.from_chain_type(llm=llm, retriever=retriever),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "OPENAI_FUNCTIONS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-0613\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     agent\u001b[39m=\u001b[39mAgentType\u001b[39m.\u001b[39;49mOPENAI_FUNCTIONS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     tools\u001b[39m=\u001b[39mtools,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/desot1/Dev/automating-metadata/app/LC_Structuredout.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m agent({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdid alphabet or tesla have more revenue?\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/enum.py:437\u001b[0m, in \u001b[0;36mEnumMeta.__getattr__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_member_map_[name]\n\u001b[1;32m    436\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: OPENAI_FUNCTIONS"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo-0613\",\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent({\"input\": \"did alphabet or tesla have more revenue?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

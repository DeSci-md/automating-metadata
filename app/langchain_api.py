"""
Use of openai plus langchain for processing information in a pdf
Generated using chatGPT for incorporating asyncio for concurrent running of prompts
Generated by pasting my code from the analysis_v3 script with the following question:
Can you modify the below python code to incorporate asyncio to allow concurrent running of the paper_search() function?
"""
import sys 
import os
from pathlib import Path  # directory setting
import asyncio # For async asking of prompts
<<<<<<< HEAD
#import httpx  # for elsevier and semantic scholar api calls
import json

import requests
import io
import PyPDF2

=======
import json

import httpx  # for elsevier and semantic scholar api calls
>>>>>>> ca6f03c1a9b6adae3ba8f0d49657242ae9308fee
from habanero import Crossref, cn  # Crossref database accessing
from dotenv import load_dotenv, find_dotenv  # loading in API keys
from langchain.document_loaders import PyPDFLoader, PyMuPDFLoader, OnlinePDFLoader # document loader import
from langchain.chat_models import ChatOpenAI  # LLM import
from langchain import LLMChain  # Agent import
from langchain.prompts.chat import ( # prompts for designing inputs
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate
)
from langchain.indexes import VectorstoreIndexCreator
import fitz #pdf reading library
import json
<<<<<<< HEAD
#from demo import read_single

#from ..Server.PDFDataExtractor.pdfdataextractor.demo import read_single
import sys 
import os
sys.path.append(os.path.abspath("/Users/desot1/Dev/automating-metadata/Server/PDFDataExtractor/pdfdataextractor"))

from demo import read_single
from dotenv import load_dotenv, find_dotenv  # loading in API keys
from pyalex import Works #Authors, Sources, Institutions, Concepts, Publishers, Funders
=======
from pyalex import Works #, Authors, Sources, Institutions, Concepts, Publishers, Funders
>>>>>>> ca6f03c1a9b6adae3ba8f0d49657242ae9308fee
import pyalex
from demo import read_single 

#from ..Server.PDFDataExtractor.pdfdataextractor.demo import read_single
sys.path.append(os.path.abspath("/Users/desot1/Dev/automating-metadata/Server/PDFDataExtractor/pdfdataextractor"))
pyalex.config.email = "ellie@desci.com"

# Load in API keys from .env file
load_dotenv(find_dotenv())


def openalex(doi): 
    works = Works()[doi]
    #specific queries are works['authorships'] and works['license']
    return works
def getdpidpdf(): 
    #Stupid ping - I will need to write a function for this - but might want to take it from Sina
    
    url = "https://beta.dpid.org/api/v1/dpid"
    response = requests.get(url)
    response_json = response.json()
    node = response_json[0]['dpid']
    base = "https://beta.dpid.org/"
    root = "?raw"

<<<<<<< HEAD
    #return most recent node JSON
    response2 = requests.get(base+node+root).json()
    
    #get the CID associated with the Payload + PDF object
    try:
        pdf_path = next(item['payload']['path'] for item in response2['components'] if item['type'] == 'pdf')
        print(pdf_path)
        
        pdf_url = next(item['payload']['url'] for item in response2['components'] if item['type'] == 'pdf')

    except: 
        print("No PDF object found")
    
    url = base+pdf_path+"?raw"
    print(url)
    ipfs="https://ipfs.desci.com/ipfs/"+pdf_url
    print(ipfs)
  
    response3 = requests.get(ipfs) 
    
    pdf_file = io.BytesIO(response3.content)

    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
    # Your PDF extraction code here
    except PyPDF2.errors.PdfReadError as e:
        print(f"Error extracting PDF: {e}")
    
    
    pdf_writer = PyPDF2.PdfWriter()

    # Assuming there's only one page in the PDF, if not, loop through pages.
    for page_num in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_num]
        pdf_writer.add_page(page)
    
    #save the PDF
    with open('/Users/desot1/Dev/automating-metadata/report.pdf', 'wb') as pdf_output:
        pdf_writer.write(pdf_output)

    """pdf_loader = UnstructuredPDFLoader('/Users/desot1/Dev/automating-metadata/report.pdf')
    text_content = pdf_loader.pdf_to_text(response3)
    print(text_content)"""
    return pdf_output.name
=======

>>>>>>> ca6f03c1a9b6adae3ba8f0d49657242ae9308fee
def paper_data_json_single(doi):
    """
    Create a json output file for a single paper using the inputed identifier.
    Only using a DOI string at the moment
    File constructed based on the info in metadata_formatting_categories.md

    Inputs:
    doi - string, DOI string for the paper/publication of interest
    output - string, path of where to save json output
    ---
    output:
    dictionary, conversion to json and writing to file
    """
    #%% Setting up info for usage of API's
    # define crossref object
    cr = Crossref()  
    cr.mailto = 'desotaelianna@gmail.com'
    cr.ua_string = 'Python/Flask script for use in Desci Nodes publication information retrieval.'

    # Elsevier API key
    apikey = os.getenv("apikey")
    client = httpx.Client()


    #%% Info from Crossref
    r = cr.works(ids = f'{doi}')  # Crossref search using DOI, "r" for request

    title = r['message']['title'][0]
    type = r['message']['type']
    pub_name = r['message']['container-title'][0]
    pub_date = r['message']['published']['date-parts'][0]
    #subject = r['message']['subject']

    inst_names = []  # handling multiple colleges, universities
    authors = []  # for handling multiple authors

    for i in r['message']['author']:
        authors.append(i['given'] + ' ' + i['family'])
        try:
            name = (i['affiliation'][0]['name'])
            if name not in inst_names:
                inst_names.append(name)
        except:
            continue

    if len(inst_names) == 0:  # returning message if no institutions returned by Crossref, may be able to get with LLM
        inst_names = "No institutions returned by CrossRef"


    refs = []
    for i in r['message']['reference']:
        try:
            refs.append(i['DOI'])
        except:
            refs.append(f"{i['key']}, DOI not present")
        
    url_link = r['message']['URL']
    

    #%% Info from Elsevier
    format = 'application/json'
    view ="FULL"
    url = f"https://api.elsevier.com/content/article/doi/{doi}?APIKey={apikey}&httpAccept={format}&view={view}"
    with httpx.Client() as client:
        r=client.get(url)
    
    json_string = r.text
    d = json.loads(json_string)  # "d" for dictionary

    try:
        d['full-text-retrieval-response']
        scopus_id = d['full-text-retrieval-response']['scopus-id']
        abstract = d['full-text-retrieval-response']['coredata']['dc:description']

        """keywords = []
        for i in d['full-text-retrieval-response']['coredata']['dcterms:subject']:
            keywords.append(i['$'])"""

        original_text = d['full-text-retrieval-response']['originalText']
    except:
        scopus_id = 'None, elsevier error'
        abstract = 'None, elsevier error'
        keywords = ['None, elsevier error']
        original_text = 'None, elsevier error'
    

    #%% Info from Semantic Scholar
    url = f'https://api.semanticscholar.org/graph/v1/paper/{doi}/?fields=fieldsOfStudy,tldr,openAccessPdf'
    with httpx.Client() as client:
        r = client.get(url)

    json_string = r.text
    d = json.loads(json_string)

    paper_id = d['paperId']

    field_of_study = []
    if d['fieldsOfStudy'] is None:
        field_of_study = 'None'
    else:
        for i in d['fieldsOfStudy']:
            field_of_study.append(i)
    if d['tldr'] is None:
        tldr = 'None'
    else:
        tldr = d['tldr']
    
    if d['openAccessPdf'] is None:
        openaccess_pdf = 'None'
    else:
        openaccess_pdf = d['openAccessPdf']['url']


    #%% Constructing output dictionary
    output_dict = {
        # Paper Metadata
        'title':title,
        'authors':authors,
        #'abstract':abstract,
        #'scopus_id':scopus_id,
        'paperId':paper_id,
        'publication_name':pub_name,
        'publish_date':pub_date,
        'type':type,
        #'keywords':keywords,
        #'subject':subject,
        'fields_of_study':field_of_study,
        'institution_names':inst_names,
        'references':refs,
        'tldr':tldr,
        #'original_text':original_text,
        'openAccessPdf':openaccess_pdf,
        'URL_link':url_link 
    }
   
    return output_dict


async def async_paper_search(query, docs, chain):
    """
    Async version of paper search, run question for the document concurrently with other questions
    """
    out = await chain.arun(doc_text=docs, query=query)  # need to have await combined with chain.arun

    return out


async def langchain_paper_search(file_path):
    """
    Analyzes a pdf document defined by file_path and asks questions regarding the text
    using LLM's.
    The results are returned as unstructured text in a dictionary.
    """
    #%% Setup, defining framework of paper info extraction
    # Define language model to use
    llm = ChatOpenAI(model_name="gpt-3.5-turbo-16k", temperature=0)

<<<<<<< HEAD
    # Structured Output Schema
    motivation_schema = ResponseSchema(name="motivation", description="This is the question or challenge that the work of this paper seeks to address.")
    methods_schema = ResponseSchema(name="methods", description="This is the experimental methods and characterization techniques used by the authors in this paper.")
    results_schema = ResponseSchema(name="results", description="This is a summary of the major results and conclusions obtained in the paper.")
    figure_schema = ResponseSchema(name="figures", description="This is a comma separated list of descriptions for each figure in the paper.")
    future_work_schema = ResponseSchema(name="future", description="This is any remaining questions or future work described by the authors in the Conclusions section of the paper.")
    title_schema = ResponseSchema(name="title", description="This is a list of the authors of this paper.")


=======
>>>>>>> ca6f03c1a9b6adae3ba8f0d49657242ae9308fee
    # Defining system and human prompts with variables
    system_template = "You are a world class research assistant who produces answers based on facts. \
                        You are tasked with reading the following publication text and answering questions based on the information: {doc_text}.\
                        You do not make up information that cannot be found in the text of the provided paper."

    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)  # providing the system prompt

    human_template = "{query}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

    chain = LLMChain(llm=llm, prompt=chat_prompt)


    #%% Extracting info from paper
    # Define the PDF document, load it in
    loader = PyPDFLoader(str(file_path))  # convert path to string to work with loader
    document = loader.load()

    # Define all the queries and corresponding schemas in a list
    queries_schemas_docs = [
        ("What are the experimental methods and techniques used by the authors? This can include ways that data was collected as well as ways the samples were synthesized.", document),
        ("What is the scientific question, challenge, or motivation that the authors are trying to address?", document),
        ("Provide a summary of the results and discussions in the paper. What results were obtained and what conclusions were reached?", document),
        ("Provide a summary of each figure described in the paper. Your response should be a one sentence summary of the figure description, \
<<<<<<< HEAD
         beginning with 'Fig. #  - description...', with each figure description separated by a comma. For example:'Fig. 1 - description..., Fig. 2 - description..., Fig. 3 - description...'", [figure_schema], document),
        ("What future work or unanswered questions are mentioned by the authors?", [future_work_schema], document),
        ("Who is(are) the author(s) of this paper?", [title_schema], document)
=======
         beginning with 'Fig. #  - description...'. For example:'Fig. 1 - description..., Fig. 2 - description..., Fig. 3 - description...'. Separate each figure description by a single newline.", document),
        ("What future work or unanswered questions are mentioned by the authors?", document),
>>>>>>> ca6f03c1a9b6adae3ba8f0d49657242ae9308fee
    ]

    tasks = []

    # Run the queries concurrently using asyncio.gather
    for query, docs in queries_schemas_docs:
        task = async_paper_search(query, docs, chain)
        tasks.append(task)

    summary = await asyncio.gather(*tasks)

    # Extracting individual elements from the summary
<<<<<<< HEAD
    # title, authors, materials, methods, motive, results, figures, future, tags = summary
    methods, motive, results, figures, future, title = summary

    llm_output = motive | methods | figures | results | future | title
=======
    methods, motive, results, figures, future = summary  #NOTE: output to variables in strings

    llm_output = {
        "motive": motive,
        "method": methods,
        "figures": figures,
        "results": results,
        "future": future
    }

    llm_output['figures'] = llm_output['figures'].split("\n")  # using newline character as a split point.
>>>>>>> ca6f03c1a9b6adae3ba8f0d49657242ae9308fee

    return llm_output


def pdfprocess(file_path): 
    results = pdfMetadata(file_path)
    print(results)
    #langchain = asyncio.run(langchain_paper_search(file_path))
    #print(langchain)
    return results
    

def pdfMetadata(file_path): 
    """
    This returns basic descriptive metadata for the PDF. 

    VARS: 
        Filepath: the path of the file you want to upload. 

    RETURNS: 
        metadata: This is the basic function of the Fitz library. 
        It scrapes the PDF for any embedded metadata. 
    """
    doc = fitz.open(file_path)
    metadata = doc.metadata
        
    #format, encryption, title, author, subject, keywords, creator, producer, creationDate, modDate, trapped
    
    secondary = read_single(file_path)
    
    #there's a chance that these never evaluate to false. Unsure why that is 
    if metadata['author'] == '' and secondary['author'] != '': 
        metadata['author'] == secondary['author']
        
    del secondary['author']

    if metadata['keywords'] == '' and secondary['keywords'] != '': 
        metadata['keywords'] == secondary['keywords']

    del secondary['keywords']
        
    metadata.update(secondary) 
    
   # if metadata['author'] == 'null': 
        #metadata['author'] == read.read_file(filepath)
    print(metadata)
    return metadata 


def results(paper_doi, pdf_path):

    #paper_doi = "10.1007/s13391-015-5352-y"
    #pdf_path = Path("s13391-015-5352-y-1.pdf")  # defining the location of the PDF file

    #%% Looking up info in databases
    api_lookup_results = paper_data_json_single(paper_doi)

    #%% Paper analysis using LangChain
    # Setting up the input and output folders
    # Load in API keys from .env file
    load_dotenv(find_dotenv())

    # Running summarization for a single document
    llm_output = asyncio.run(langchain_paper_search(pdf_path))

    # Constructing the final output
    final_output = api_lookup_results | llm_output

<<<<<<< HEAD
if __name__=="__main__": 
    llm_output = asyncio.run(langchain_paper_search(getdpidpdf()))
    #print(llm_output)
    #print(openalex("https://doi.org/http://dx.doi.org/10.18725/OPARU-3732"))
    
=======

#%% Main, general case for testing
if __name__ == "__main__":
    print("Starting code run...")
    cwd = Path(__file__)
    pdf_folder = cwd.parents[1].joinpath('.test_pdf')  # path to the folder containing the pdf to test

    # File name of pdf in the .test_pdf folder for testing with code
    file_name = "1087792.pdf"  # test 1
    # file_name = "Zhao et al_2023_Homonuclear dual-atom catalysts embedded on N-doped graphene for highly.pdf"  # test 2, too long
    # file_name = "Ren_Dong_2022_Direct electrohydrodynamic printing of aqueous silver nanowires ink on.pdf"  # test 3
    # file_name = "Chang et al_2022_Few-layer graphene as an additive in negative electrodes for lead-acid batteries.pdf"  # test 4
    # file_name = "Zhang et al_2019_Highly Stretchable Patternable Conductive Circuits and Wearable Strain Sensors.pdf"  # test 5
    # file_name = "Jepsen_2019_Phase Retrieval in Terahertz Time-Domain Measurements.pdf"  # test 6

    pdf_file_path = pdf_folder.joinpath(file_name)

    llm_output = asyncio.run(langchain_paper_search(pdf_file_path))  # output of unstructured text in dictionary

    print(llm_output['figures'])

    print("Script completed")
>>>>>>> ca6f03c1a9b6adae3ba8f0d49657242ae9308fee

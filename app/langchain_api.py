"""
Use of openai plus langchain for processing information in a pdf
Generated using chatGPT for incorporating asyncio for concurrent running of prompts
Generated by pasting my code from the analysis_v3 script with the following question:
Can you modify the below python code to incorporate asyncio to allow concurrent running of the paper_search() function?
"""
import sys 
import requests
import os
from pathlib import Path  # directory setting
import asyncio # For async asking of prompts
import json
from langchain.docstore.document import Document

import httpx  # for elsevier and semantic scholar api calls
from habanero import Crossref, cn  # Crossref database accessing
from dotenv import load_dotenv, find_dotenv  # loading in API keys
from langchain.document_loaders import PyPDFLoader # document loader import
from langchain.chat_models import ChatOpenAI  # LLM import
from langchain import LLMChain  # Agent import
from langchain.prompts.chat import ( # prompts for designing inputs
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate
)
from langchain.indexes import VectorstoreIndexCreator
import fitz #pdf reading library
import json
from pyalex import Works #, Authors, Sources, Institutions, Concepts, Publishers, Funders
import pyalex
import PyPDF2
import io
#from demo import read_single 

#TODO: IF doi -> then search open alex -> determine relevant metadata to return. -> Together once everything is up to date. 
#TODO: Combine Paper_data_Json_Single + Open Alex -> into a database_search -> to get external data. - Henry
#TODO: get api + langchain + sturcutred output in a pretty package -> Ellie
#TODO: Dockerize -> Ellie. 

#from ..Server.PDFDataExtractor.pdfdataextractor.demo import read_single
sys.path.append(os.path.abspath("/Users/desot1/Dev/automating-metadata/Server/PDFDataExtractor/pdfdataextractor"))
pyalex.config.email = "ellie@desci.com"

# Load in API keys from .env file
load_dotenv(find_dotenv())
import tiktoken

def num_tokens_from_string(string: str, encoding_name: str) -> int:
    encoding = tiktoken.encoding_for_model(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

print(num_tokens_from_string("Hello world, let's test tiktoken.", "gpt-3.5-turbo"))

def get_jsonld(node):
    base = "https://beta.dpid.org/"
    root = "?jsonld"

    #return requested node JSON
    response2 = requests.get(base+node+root).json()
    return response2

def get_pdf_text(node):
    base = "https://beta.dpid.org/"
    root = "?raw"

    #return most recent node JSON
    manifest = requests.get(base+node+root).json()
    
    #get the CID associated with the Payload + PDF object
    try:
        pdf_path = next(item['payload']['path'] for item in manifest['components'] if item['type'] == 'pdf')
        print(pdf_path)
        
        pdf_url = next(item['payload']['url'] for item in manifest['components'] if item['type'] == 'pdf')

    except: 
        return "No PDF object found"
    
    url = base+pdf_path+"?raw"
    print(url)
    ipfs="https://ipfs.desci.com/ipfs/"+pdf_url
    print(ipfs)
  
    response = requests.get(ipfs) 
    
    if response.status_code == 200:
        # Open the PDF content with PyPDF2
        pdf_file = PyPDF2.PdfReader(io.BytesIO(response.content))
        
        # Check if the PDF is extractable
        if pdf_file.is_encrypted:
            pdf_file.decrypt("")  # Assuming the PDF has no password
            
        
        # Initialize text
        pdf_text = ""
        
        # Limit processing to 7 pages
        num_pages = min(len(pdf_file.pages), 7)
        
        for i in range(num_pages):
            page = pdf_file.pages[i]
            page_text = page.extract_text()
            
            if page_text is not None:
                pdf_text += page_text
            
        return pdf_text
    else:
        print(f"Error fetching PDF from {url}. Status code: {response.status_code}")
        return None

def paper_data_json_single(doi):
    """
    Create a json output file for a single paper using the inputed identifier.
    Only using a DOI string at the moment
    File constructed based on the info in metadata_formatting_categories.md

    Inputs:
    doi - string, DOI string for the paper/publication of interest
    output - string, path of where to save json output
    ---
    output:
    dictionary, conversion to json and writing to file
    """
    #%% Setting up info for usage of API's
    # define crossref object
    cr = Crossref()  
    cr.mailto = 'desotaelianna@gmail.com'
    cr.ua_string = 'Python/Flask script for use in Desci Nodes publication information retrieval.'

    # Elsevier API key
    apikey = os.getenv("apikey")
    client = httpx.Client()


    #%% Info from Crossref
    try:
        r = cr.works(ids = f'{doi}')  # Crossref search using DOI, "r" for request
    except requests.exceptions.HTTPError as e:
        print(f"CrossRef DOI lookup returned error: {e}\n")

    try:
        title = r['message']['title'][0]
    except:
        title = f"None, Crossref Error"

    try:
        type = r['message']['type']
    except:
        type = "None, Crossref Error"

    try:
        pub_name = r['message']['container-title'][0]
    except:
        pub_name = "None, Crossref Error"

    try:
        pub_date = r['message']['published']['date-parts'][0]
    except:
        pub_date = "None, Crossref Error"

    try:
        subject = r['message']['subject']
    except:
        subject = "None, Crossref Error"


    inst_names = []  # handling multiple colleges, universities
    authors = []  # for handling multiple authors

    for i in r['message']['author']:
        authors.append(i['given'] + ' ' + i['family'])
        try:
            name = (i['affiliation'][0]['name'])
            if name not in inst_names:
                inst_names.append(name)
        except:
            continue

    if len(inst_names) == 0:  # returning message if no institutions returned by Crossref, may be able to get with LLM
        inst_names = "No institutions returned by CrossRef"


    refs = []
    for i in r['message']['reference']:
        try:
            refs.append(i['DOI'])
        except:
            refs.append(f"{i['key']}, DOI not present")
        
    url_link = r['message']['URL']
    

    #%% Info from Elsevier
    format = 'application/json'
    view ="FULL"
    url = f"https://api.elsevier.com/content/article/doi/{doi}?APIKey={apikey}&httpAccept={format}&view={view}"
    with httpx.Client() as client:
        r=client.get(url)
    
    json_string = r.text
    d = json.loads(json_string)  # "d" for dictionary

    try:
        scopus_id = d['full-text-retrieval-response']['scopus-id']
    except:
        scopus_id = 'None, elsevier error'

    try:
        abstract = d['full-text-retrieval-response']['coredata']['dc:description']
    except:
        abstract = 'None, elsevier error'

    try:
        keywords = []
        for i in d['full-text-retrieval-response']['coredata']['dcterms:subject']:
            keywords.append(i['$'])

    except:
        keywords = ['None, elsevier error']

    try:
        original_text = d['full-text-retrieval-response']['originalText']
    except:
        original_text = 'None, elsevier error'
    

    #%% Info from Semantic Scholar
    url = f'https://api.semanticscholar.org/graph/v1/paper/{doi}/?fields=fieldsOfStudy,tldr,openAccessPdf'
    with httpx.Client() as client:
        r = client.get(url)

    json_string = r.text
    d = json.loads(json_string)

    try:
        paper_id = d['paperId']
    except:
        paper_id = "None, Semantic Scholar lookup error"

    field_of_study = []
    try:
        if d['fieldsOfStudy'] is None:
            field_of_study = 'None'
        else:
            for i in d['fieldsOfStudy']:
                field_of_study.append(i)
    except:
        field_of_study = "None, Semantic Scholar lookup error"

    try:
        if d['tldr'] is None:
            tldr = 'None'
        else:
            tldr = d['tldr']
    except:
        tldr = "None, Semantic Scholar lookup error"

    try:
        if d['openAccessPdf'] is None:
            openaccess_pdf = 'None'
        else:
            openaccess_pdf = d['openAccessPdf']['url']
    except:
        openaccess_pdf = "None, Semantic Scholar lookup error"

    # OpenAlex accessing as backup info for the previous tools
    openalex_results = Works()[doi]
    try:
        openalex_id = openalex_results['id']
    except: 
        openalex_id = "None, OpenAlex Lookup error"

    if "Error" in title:  # attempt replacing error title from cross with title from openalex
        try:
            title = openalex_results['title']
        except:
            pass
    if "Error" in type:  # attempt replacing error keywords from cross with title from openalex
        try:
            type = openalex_results['type']
        except:
            pass
    if "Error" in pub_name:  # attempt replacing error keywords from cross with title from openalex
        try:
            pub_name = openalex_results['primary_location']
        except:
            pass

    if "Error" in pub_date:  # attempt replacing error keywords from cross with title from openalex
        try:
            pub_date = openalex_results['publication_date']
        except:
            pass

    if "Error" in keywords:  # attempt replacing error keywords from cross with title from openalex
        try:
            title = openalex_results['title']
        except:
            pass
    
    try:
        openaccess = openalex_results['open_access']
    except:
        pass
    #%% Constructing output dictionary
    output_dict = {
        # Paper Metadata
        'title':title,
        'authors':authors,
        'abstract':abstract,
        'scopus_id':scopus_id,
        'paperId':paper_id,
        'publication_name':pub_name,
        'publish_date':pub_date,
        'type':type,
        'keywords':keywords,
        'subject':subject,
        'fields_of_study':field_of_study,
        'institution_names':inst_names,
        'references':refs,
        'tldr':tldr,
        'original_text':original_text,
        'openAccessPdf':openaccess_pdf,
        'URL_link': url_link,
        'openalex_id': openalex_id,
        'openaccess': openaccess
    }
   
    return output_dict

async def langchain_paper_search(node):
    #file_path
    """
    Analyzes a pdf document defined by file_path and asks questions regarding the text
    using LLM's.
    The results are returned as unstructured text in a dictionary.
    """
    #%% Setup, defining framework of paper info extraction
    # Define language model to use
    llm = ChatOpenAI(model_name="gpt-3.5-turbo-16k", temperature=0)

    # Defining system and human prompts with variables
    system_template = "You are a world class research assistant who produces answers based on facts. \
                        You are tasked with reading the following publication text and answering questions based on the information: {doc_text}.\
                        You do not make up information that cannot be found in the text of the provided paper."

    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)  # providing the system prompt

    human_template = "{query}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

    chain = LLMChain(llm=llm, prompt=chat_prompt)


    #%% Extracting info from paper
    # Define the PDF document, load it in
    text = get_pdf_text(node)
    document = Document(page_content = text)

    # Define all the queries and corresponding schemas in a list
    queries_schemas_docs = [
        #("What are the experimental methods and techniques used by the authors? This can include ways that data was collected as well as ways the samples were synthesized.", document),
        #("What is the scientific question, challenge, or motivation that the authors are trying to address?", document),
        #("Provide a summary of the results and discussions in the paper. What results were obtained and what conclusions were reached?", document),
        ("Provide a summary of each figure described in the paper. Your response should be a one sentence summary of the figure description, \
         beginning with 'Fig. #  - description...'. For example:'Fig. 1 - description..., Fig. 2 - description..., Fig. 3 - description...'. Separate each figure description by a single newline.", document),
        #("What future work or unanswered questions are mentioned by the authors?", document),
        ("Tell me who all the authors of this paper are. Your response should be a comma separated list of the authors of the paper, \
         looking like 'first author name, second author name", document),
        ("Tell me the title of this paper", document)
    ]

    tasks = []

    # Run the queries concurrently using asyncio.gather
    for query, docs in queries_schemas_docs:
        task = chain.arun(doc_text=docs, query=query)
        tasks.append(task)

    summary = await asyncio.gather(*tasks)

    # Extracting individual elements from the summary
    #methods, motive, results, future
    figures, authors, title = summary  #NOTE: output to variables in strings

    llm_output = {
        #"motive": motive,
        #"method": methods,
        "figures": figures,
        #"results": results,
        #"future": future,
        "authors": authors,
        "title": title
    }

    #transform outputs
    llm_output['figures'] = llm_output['figures'].split("\n")# using newline character as a split point.
    llm_output['authors'] = llm_output['authors'].split(', ') 
    llm_output['authors'] = get_orcid(llm_output["authors"])

    return llm_output

def get_orcid(authors): 
    orcid = []
    author_info = {}   

    for author in authors: 
        try: 
            url = "https://api.openalex.org/autocomplete/authors?q=" + author
            response = json.loads(requests.get(url).text)
        except: 
            print(f"OpenAlex ORCID lookup returned error: {e}\n")
        
        if response["meta"]["count"] == 1: 
            orcid = response["results"][0]["external_id"]
            author_info[author] = {"orcid": orcid, "affiliation":response["results"][0]["hint"]}
        elif response["meta"]["count"] == 0: #FAKE - Create a test so we can check if the return is valid. 
            print("There are no OrcID suggestions for this author")
        else: 
            orcid = response["results"][0]["external_id"]
            author_info[author] = {"orcid": orcid, "affiliation": response["results"][0]["hint"]}
            #create an async function which ranks the authors based on the similarity to the paper. 

    return author_info

def update_json_ld(json_ld, new_data):
    # Process author information
    for key, value in new_data.items(): 
        if key == "authors": 
            for author_name, author_info in new_data.get("authors", {}).items():
                orchid = author_info.get("orcid")
                institution = author_info.get("affiliation")

                creator_entry = {
                    "@type": "Person",
                    "name": author_name
                }

                if orchid:
                    creator_entry["@id"] = {"@id": orchid}

                if institution:
                    creator_entry["affiliation"] = institution

                json_ld["@graph"].append(creator_entry)
                json_ld["@graph"][1]["creator"].append(creator_entry)
        else:
            json_ld["@graph"][1][key.lower()] = value

        return json_ld


#%% Main, general case for testing
if __name__ == "__main__":
    print("Starting code run...")

    node = os.getenv('NODE_ENV')
    DOI_env = os.getenv('DOI_ENV')
    
    if node is not None:
        print(f"NODE_ENVIRONMENT is set to: {node}")
    else:
        print("NODE_ENVIRONMENT is not set.")

    json_ld = get_jsonld(node)

    if DOI_env: 
        lookup_results = paper_data_json_single(DOI_env)
        updated_json_ld = update_json_ld(json_ld, lookup_results)
    else: 
        updated_json_ld = json_ld

    llm_output = asyncio.run(langchain_paper_search(node))# output of unstructured text in dictionary
    updated_json_ld = update_json_ld(json_ld, llm_output)
   
    #doi = "https://doi.org/10.1002/adma.202208113"
    
    print(updated_json_ld)

    print("Script completed")
